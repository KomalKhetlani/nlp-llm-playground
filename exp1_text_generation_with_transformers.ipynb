{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbextension disable --py widgetsnbextension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yHzqc7kUAzk",
        "outputId": "59cf2b45-fc94-4317-b213-462517440a7a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disabling notebook extension jupyter-js-widgets/extension...\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook demonstrates how to load instruction-tuned models (like `Phi-3-mini`, `Falcon-7B-Instruct`) using the Hugging Face `transformers` library, create a text-generation pipeline, and experiment with prompts to generate human-like responses.**\n"
      ],
      "metadata": {
        "id": "d6oBOo9N44lw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6-wMv6Ba3e79"
      },
      "outputs": [],
      "source": [
        "\"\"\"Install required libraries\n",
        "We install Hugging Face's `transformers` (for pretrained NLP models)\n",
        "and `accelerate` (for efficient model loading on different hardware like CPU/GPU)\"\"\"\n",
        "!pip install transformers>=4.40.1 accelerate>=0.27.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required classes from transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\"\"\"Load the model and tokenizer\n",
        "    - AutoModelForCausalLM: loads a pre-trained model for Causal Language Modeling (text generation).\n",
        "    - AutoTokenizer: loads the matching tokenizer (converts text to tokens and back).\n",
        "    - Here we are using Microsoft's \"microsoft/Phi-3-mini-4k-instruct\", a lightweight instruction-tuned model.\n",
        "    - device_map=\"cuda\": ensures the model runs on GPU if available.\n",
        "    - torch_dtype=\"auto\": lets PyTorch automatically select the best precision (e.g., float16/32).\n",
        "    - trust_remote_code=False: avoids running custom model code for security reasons.\"\"\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "854befd533ae49c4ba790f4669eaa580",
            "a725a7c7b3454ef6b5fcd08f6e424802",
            "185885802417440f8d16604aa5fcf700",
            "0988648c4da944b4822beb447fbf3905",
            "e274d56b92144163b6bf5092011c57ad",
            "2440e551de5e451da6caf44023b36e66",
            "d8603f395abc4706a3127a0b994c6e1e",
            "eb59a402e8d64b388d54e63ab0619554",
            "fe03175bdb6c43e696523fc4a880d34d",
            "834cf18733ef4bfeaf8782594929c485",
            "498914320eac4971b1470aaf0f2697e0"
          ]
        },
        "id": "mmt-imCm3uMK",
        "outputId": "ad7dbff6-6d93-46a5-8982-b809cbf5a1d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "854befd533ae49c4ba790f4669eaa580"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Hugging Face's high-level pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "\"\"\"Create a text generation pipeline\n",
        "  Why use pipeline?\n",
        "    - It abstracts away low-level details (tokenization, model forwarding, decoding).\n",
        "    - Easy to use: you just provide the model, tokenizer, and parameters.\n",
        "  Parameters explained:\n",
        "    - task=\"text-generation\": we want the model to generate text.\n",
        "    - return_full_text=False: ensures we only get the generated continuation, not the original prompt.\n",
        "    - max_new_tokens=500: limits how long the generated output can be.\n",
        "    - do_sample=False: disables randomness (deterministic output), good for reproducibility.\"\"\"\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "id": "UEPjCLyh4Yg3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ed80d9-c5e2-4d5d-ee52-926a1de3cf29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt (user query)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Summarize the importance of renewable energy in 3 bullet points.\"}\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "zSPbNTK_4wVe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Generate output from the model\n",
        "  - We pass the messages to the pipeline.\n",
        "  - The pipeline internally handles tokenization, model inference, and decoding.\n",
        "  - Response is returned as a list of dictionaries, where \"generated_text\" contains the response\"\"\"\n",
        "response = generator(messages)\n",
        "print(response[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "0efKv5Rj4yK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "960eb1bf-f54d-4efd-e211-4fd081111125"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - Renewable energy sources, such as solar, wind, and hydro, are essential for reducing greenhouse gas emissions and combating climate change.\n",
            "- They provide a sustainable and inexhaustible supply of energy, reducing dependence on finite fossil fuels and enhancing energy security.\n",
            "- Investing in renewable energy technologies creates jobs, stimulates economic growth, and promotes innovation in the energy sector.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Pdk10Sf340sS"
      }
    }
  ]
}
