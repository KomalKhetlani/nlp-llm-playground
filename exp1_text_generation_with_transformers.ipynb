{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1759752129904,
     "user": {
      "displayName": "Coco C",
      "userId": "10569321822159683379"
     },
     "user_tz": -330
    },
    "id": "5yHzqc7kUAzk",
    "outputId": "59cf2b45-fc94-4317-b213-462517440a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension disable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6oBOo9N44lw"
   },
   "source": [
    "**This notebook demonstrates how to load instruction-tuned models (like `Phi-3-mini`, `Falcon-7B-Instruct`) using the Hugging Face `transformers` library, create a text-generation pipeline, and experiment with prompts to generate human-like responses.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11696,
     "status": "ok",
     "timestamp": 1759751812675,
     "user": {
      "displayName": "Coco C",
      "userId": "10569321822159683379"
     },
     "user_tz": -330
    },
    "id": "6-wMv6Ba3e79"
   },
   "outputs": [],
   "source": [
    "\"\"\"Install required libraries\n",
    "We install Hugging Face's `transformers` (for pretrained NLP models)\n",
    "and `accelerate` (for efficient model loading on different hardware like CPU/GPU)\"\"\"\n",
    "!pip install transformers>=4.40.1 accelerate>=0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "854befd533ae49c4ba790f4669eaa580",
      "a725a7c7b3454ef6b5fcd08f6e424802",
      "185885802417440f8d16604aa5fcf700",
      "0988648c4da944b4822beb447fbf3905",
      "e274d56b92144163b6bf5092011c57ad",
      "2440e551de5e451da6caf44023b36e66",
      "d8603f395abc4706a3127a0b994c6e1e",
      "eb59a402e8d64b388d54e63ab0619554",
      "fe03175bdb6c43e696523fc4a880d34d",
      "834cf18733ef4bfeaf8782594929c485",
      "498914320eac4971b1470aaf0f2697e0"
     ]
    },
    "executionInfo": {
     "elapsed": 38672,
     "status": "ok",
     "timestamp": 1759751853348,
     "user": {
      "displayName": "Coco C",
      "userId": "10569321822159683379"
     },
     "user_tz": -330
    },
    "id": "mmt-imCm3uMK",
    "outputId": "ad7dbff6-6d93-46a5-8982-b809cbf5a1d4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854befd533ae49c4ba790f4669eaa580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the required classes from transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\"\"\"Load the model and tokenizer\n",
    "    - AutoModelForCausalLM: loads a pre-trained model for Causal Language Modeling (text generation).\n",
    "    - AutoTokenizer: loads the matching tokenizer (converts text to tokens and back).\n",
    "    - Here we are using Microsoft's \"microsoft/Phi-3-mini-4k-instruct\", a lightweight instruction-tuned model.\n",
    "    - device_map=\"cuda\": ensures the model runs on GPU if available.\n",
    "    - torch_dtype=\"auto\": lets PyTorch automatically select the best precision (e.g., float16/32).\n",
    "    - trust_remote_code=False: avoids running custom model code for security reasons.\"\"\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1759751877687,
     "user": {
      "displayName": "Coco C",
      "userId": "10569321822159683379"
     },
     "user_tz": -330
    },
    "id": "UEPjCLyh4Yg3",
    "outputId": "85ed80d9-c5e2-4d5d-ee52-926a1de3cf29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Hugging Face's high-level pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "\"\"\"Create a text generation pipeline\n",
    "  Why use pipeline?\n",
    "    - It abstracts away low-level details (tokenization, model forwarding, decoding).\n",
    "    - Easy to use: you just provide the model, tokenizer, and parameters.\n",
    "  Parameters explained:\n",
    "    - task=\"text-generation\": we want the model to generate text.\n",
    "    - return_full_text=False: ensures we only get the generated continuation, not the original prompt.\n",
    "    - max_new_tokens=500: limits how long the generated output can be.\n",
    "    - do_sample=False: disables randomness (deterministic output), good for reproducibility.\"\"\"\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1759751878160,
     "user": {
      "displayName": "Coco C",
      "userId": "10569321822159683379"
     },
     "user_tz": -330
    },
    "id": "zSPbNTK_4wVe"
   },
   "outputs": [],
   "source": [
    "# Define the prompt (user query)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Summarize the importance of renewable energy in 3 bullet points.\"}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4346,
     "status": "ok",
     "timestamp": 1759751882834,
     "user": {
      "displayName": "Coco C",
      "userId": "10569321822159683379"
     },
     "user_tz": -330
    },
    "id": "0efKv5Rj4yK9",
    "outputId": "960eb1bf-f54d-4efd-e211-4fd081111125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Renewable energy sources, such as solar, wind, and hydro, are essential for reducing greenhouse gas emissions and combating climate change.\n",
      "- They provide a sustainable and inexhaustible supply of energy, reducing dependence on finite fossil fuels and enhancing energy security.\n",
      "- Investing in renewable energy technologies creates jobs, stimulates economic growth, and promotes innovation in the energy sector.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate output from the model\n",
    "  - We pass the messages to the pipeline.\n",
    "  - The pipeline internally handles tokenization, model inference, and decoding.\n",
    "  - Response is returned as a list of dictionaries, where \"generated_text\" contains the response\"\"\"\n",
    "response = generator(messages)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdk10Sf340sS"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPLfCZ7ewUmqQBtuKO5NRyW",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
